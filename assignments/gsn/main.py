# -*- coding: utf-8 -*-
"""Exercise_7_Minibatch_P3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIgLdGaaNzsElHd52dIMAV13UuIkThkq

In this lab, you should try to implement some of the techniques discussed in the lecture.
Here is a list of reasonable tasks.

Easy:
 * L1 or L2 regularization (choose one)
 * momentum, Nesterov's momentum (choose one)

Medium difficulty:
 * Adagrad, RMSProp (choose one)
 * dropout
 * data augmentation (tiny rotatations, up/down-scalings etc.)

Try to test your network to see if these changes improve accuracy. They improve accuracy much more if you increase the layer size, and if you add more layers.
"""

import random
import numpy as np
from torchvision import datasets, transforms

from decimal import Decimal
# self.velWs[0].mean()

# Let's read the mnist dataset

def load_mnist(path='.'):
    train_set = datasets.MNIST(path, train=True, download=True)
    x_train = train_set.data.numpy()
    _y_train = train_set.targets.numpy()

    test_set = datasets.MNIST(path, train=False, download=True)
    x_test = test_set.data.numpy()
    _y_test = test_set.targets.numpy()

    x_train = x_train.reshape((x_train.shape[0], 28 * 28)) / 255.
    x_test = x_test.reshape((x_test.shape[0], 28 * 28)) / 255.

    y_train = np.zeros((_y_train.shape[0], 10))
    y_train[np.arange(_y_train.shape[0]), _y_train] = 1

    y_test = np.zeros((_y_test.shape[0], 10))
    y_test[np.arange(_y_test.shape[0]), _y_test] = 1

    return (x_train, y_train), (x_test, y_test)


# (x_train, y_train), (x_test, y_test) = load_mnist()
(x_train, y_train), (x_test, y_test) = load_mnist(rf"C:\Datasets")


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def sigmoid_prime(z):
    # Derivative of the sigmoid
    return sigmoid(z) * (1 - sigmoid(z))


REG_RATE = 0.01
MOMENTUM = 0.5

BUFF = []

class Network(object):
    def __init__(self, sizes):
        # initialize biases and weights with random normal distr.
        # weights are indexed by target node first
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]
        self.regularization = None
        self.momentum_type = None
        self.momentum = 0
        self.velWs = [np.zeros(layer.shape) for layer in self.weights]
        self.velBs = [np.zeros(layer.shape) for layer in self.biases]
        self.GWs = [np.zeros(layer.shape) for layer in self.weights]
        self.GBs = [np.zeros(layer.shape) for layer in self.biases]

    def feedforward(self, a):
        # Run the network on a batch
        a = a.T
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.matmul(w, a) + b)
        return a

    def update_mini_batch(self, mini_batch, eta):
        # Update networks weights and biases by applying a single step
        # of gradient descent using backpropagation to compute the gradient.
        # The gradient is computed for a mini_batch which is as in tensorflow API.
        # eta is the learning rate
        nabla_b, nabla_w = self.backprop(mini_batch[0].T, mini_batch[1].T)

        self.GWs = [GW + nw ** 2 for GW, nw in zip(self.GWs, nabla_w)]
        self.GBs = [GB + nb ** 2 for GB, nb in zip(self.GBs, nabla_b)]
        # BUFF.append(self.GWs[0].mean())

        # self.GWs[0].mean()
        # nabla_w[0].mean()
        #
        # GW_2 = self.GWs[0] ** 2
        # nabla_2 = nabla_w[0] ** 2
        # res = GW_2 + nabla_2
        #
        # GW_2.mean()
        # nabla_2.mean()
        # res.mean()

        # TODO rozbic ponizsze na dwa etapy

        self.velWs = [self.momentum * velW + (eta / len(mini_batch[0])) * nw for velW, nw in zip(self.velWs, nabla_w)]
        self.velBs = [self.momentum * velB + (eta / len(mini_batch[0])) * nb for velB, nb in zip(self.velBs, nabla_b)]

        self.weights = [w - velW for w, velW in zip(self.weights, self.velWs)]
        self.biases = [b - velB for b, velB in zip(self.biases, self.velBs)]

    def backprop(self, x, y):
        # For a single input (x,y) return a pair of lists.
        # First contains gradients over biases, second over weights.
        g = x
        gs = [g]  # list to store all the gs, layer by layer
        fs = []  # list to store all the fs, layer by layer
        for b, w in zip(self.biases, self.weights):
            f = np.dot(w, g) + b
            fs.append(f)
            g = sigmoid(f)
            gs.append(g)
        # backward pass <- both steps at once
        dLdg = self.cost_derivative(gs[-1], y)
        dLdfs = []
        for w, g in reversed(list(zip(self.weights, gs[1:]))):
            dLdf = np.multiply(dLdg, np.multiply(g, 1 - g))
            dLdfs.append(dLdf)
            dLdg = np.matmul(w.T, dLdf)

        dLdWs = [np.matmul(dLdf, g.T) for dLdf, g in zip(reversed(dLdfs), gs[:-1])]  # automatic here
        if self.regularization == 'L2':
            dLdWs = [dLdW + REG_RATE * w for dLdW, w in zip(dLdWs, self.weights)]
        dLdBs = [np.sum(dLdf, axis=1).reshape(dLdf.shape[0], 1) for dLdf in reversed(dLdfs)]  # CHANGE: Need to sum here
        return (dLdBs, dLdWs)

    def evaluate(self, test_data):
        # Count the number of correct answers for test_data
        pred = np.argmax(self.feedforward(test_data[0]), axis=0)
        corr = np.argmax(test_data[1], axis=1).T
        return np.mean(pred == corr)

    def cost_derivative(self, output_activations, y):
        return (output_activations - y)

    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):
        x_train, y_train = training_data
        if test_data:
            x_test, y_test = test_data
        if self.momentum_type is not None:
            self.momentum = MOMENTUM
        for j in range(epochs):
            for i in range(x_train.shape[0] // mini_batch_size):
                x_mini_batch = x_train[(mini_batch_size * i):(mini_batch_size * (i + 1))]
                y_mini_batch = y_train[(mini_batch_size * i):(mini_batch_size * (i + 1))]
                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)
            if test_data:
                print("Epoch: {0}, Accuracy: {1}".format(j, self.evaluate((x_test, y_test))))
            else:
                print("Epoch: {0}".format(j))

np.random.seed(0)
network = Network([784, 30, 10])
# network.regularization = 'L2'
# network.momentum_type = 'regular'
network.decay = 'adagrad'
network.SGD((x_train, y_train), epochs=10, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))


# Epoch: 0, Accuracy: 0.5713
# Epoch: 1, Accuracy: 0.6972
# Epoch: 2, Accuracy: 0.7167
# Epoch: 3, Accuracy: 0.7282
# Epoch: 4, Accuracy: 0.8319
# Epoch: 5, Accuracy: 0.9058
# Epoch: 6, Accuracy: 0.9104
# Epoch: 7, Accuracy: 0.9131
# Epoch: 8, Accuracy: 0.9161
# Epoch: 9, Accuracy: 0.9185

"""
vanilla         0.94/32
L2 0.01         0.94/10     0.95/27 
Momentum 0.5    0.94/16     
"""

# import matplotlib.pyplot as plt
# plt.scatter(range(600), BUFF)
