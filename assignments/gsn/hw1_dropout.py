# -*- coding: utf-8 -*-
"""Exercise_7_Minibatch_P3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIgLdGaaNzsElHd52dIMAV13UuIkThkq

In this lab, you should try to implement some of the techniques discussed in the lecture.
Here is a list of reasonable tasks.

Easy:
 * L1 or L2 regularization (choose one)
 * momentum, Nesterov's momentum (choose one)

Medium difficulty:
 * Adagrad, RMSProp (choose one)
 * dropout
 * data augmentation (tiny rotatations, up/down-scalings etc.)

Try to test your network to see if these changes improve accuracy. They improve accuracy much more if you increase the layer size, and if you add more layers.
"""

import random
import numpy as np
from torchvision import datasets, transforms
from assignments.gsn.transform import Reducer
import copy

# Let's read the mnist dataset

def load_mnist(path='.'):
    train_set = datasets.MNIST(path, train=True, download=True)
    x_train = train_set.data.numpy()
    _y_train = train_set.targets.numpy()

    test_set = datasets.MNIST(path, train=False, download=True)
    x_test = test_set.data.numpy()
    _y_test = test_set.targets.numpy()

    x_train = x_train.reshape((x_train.shape[0], 28 * 28)) / 255.
    x_test = x_test.reshape((x_test.shape[0], 28 * 28)) / 255.

    y_train = np.zeros((_y_train.shape[0], 10))
    y_train[np.arange(_y_train.shape[0]), _y_train] = 1

    y_test = np.zeros((_y_test.shape[0], 10))
    y_test[np.arange(_y_test.shape[0]), _y_test] = 1

    return (x_train, y_train), (x_test, y_test)




def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def sigmoid_prime(z):
    # Derivative of the sigmoid
    return sigmoid(z) * (1 - sigmoid(z))


class Network(object):
    def __init__(self, sizes):
        # initialize biases and weights with random normal distr.
        # weights are indexed by target node first
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]

    def feedforward(self, a):
        # Run the network on a batch
        a = a.T
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.matmul(w, a) + b)
        return a

    def update_mini_batch(self, mini_batch, eta):
        # Update networks weights and biases by applying a single step
        # of gradient descent using backpropagation to compute the gradient.
        # The gradient is computed for a mini_batch which is as in tensorflow API.
        # eta is the learning rate
        nabla_b, nabla_w = self.backprop(mini_batch[0].T, mini_batch[1].T)

        self.weights = [w - (eta / len(mini_batch[0])) * nw for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b - (eta / len(mini_batch[0])) * nb for b, nb in zip(self.biases, nabla_b)]

    def backprop(self, x, y):
        # For a single input (x,y) return a pair of lists.
        # First contains gradients over biases, second over weights.
        g = x
        gs = [g]  # list to store all the gs, layer by layer
        fs = []  # list to store all the fs, layer by layer


        weights = copy.deepcopy(self.weights)
        biases = copy.deepcopy(self.biases)

        # Size reduction
        layer_size = self.weights[0].shape[0]
        survivor_list = np.random.choice(layer_size, int(layer_size/2), replace=False)
        rdcr = Reducer().fit(survivor_list, layer_size)
        weights[0], biases[0], weights[1] = rdcr.transform(weights[0], biases[0], weights[1])

        for b, w in zip(biases, weights):
            f = np.dot(w, g) + b
            fs.append(f)
            g = sigmoid(f)
            gs.append(g)
        # backward pass <- both steps at once
        dLdg = self.cost_derivative(gs[-1], y)
        dLdfs = []
        for w, g in reversed(list(zip(weights, gs[1:]))):
            dLdf = np.multiply(dLdg, np.multiply(g, 1 - g))
            dLdfs.append(dLdf)
            dLdg = np.matmul(w.T, dLdf)

        dLdWs = [np.matmul(dLdf, g.T) for dLdf, g in zip(reversed(dLdfs), gs[:-1])]  # automatic here
        dLdBs = [np.sum(dLdf, axis=1).reshape(dLdf.shape[0], 1) for dLdf in reversed(dLdfs)]  # CHANGE: Need to sum here
        # Back to previous size.
        dLdWs[0], dLdBs[0], dLdWs[1] = rdcr.inverse_transform(dLdWs[0], dLdBs[0], dLdWs[1])
        return (dLdBs, dLdWs)

    def evaluate(self, test_data):
        # Count the number of correct answers for test_data
        pred = np.argmax(self.feedforward(test_data[0]), axis=0)
        corr = np.argmax(test_data[1], axis=1).T
        return np.mean(pred == corr)

    def cost_derivative(self, output_activations, y):
        return (output_activations - y)

    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):
        x_train, y_train = training_data
        if test_data:
            x_test, y_test = test_data
        for j in range(epochs):
            for i in range(x_train.shape[0] // mini_batch_size):
                x_mini_batch = x_train[(mini_batch_size * i):(mini_batch_size * (i + 1))]
                y_mini_batch = y_train[(mini_batch_size * i):(mini_batch_size * (i + 1))]
                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)
            if test_data:
                print("Epoch: {0}, Accuracy: {1}".format(j, self.evaluate((x_test, y_test))))
            else:
                print("Epoch: {0}".format(j))

(x_train, y_train), (x_test, y_test) = load_mnist(rf"C:\Datasets")
np.random.seed(0)
network = Network([784, 30, 10])
network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))

# Epoch: 0, Accuracy: 0.4581
# Epoch: 1, Accuracy: 0.7381
# Epoch: 2, Accuracy: 0.8036
# Epoch: 3, Accuracy: 0.8137
# Epoch: 4, Accuracy: 0.8326
# Epoch: 5, Accuracy: 0.8449
# Epoch: 6, Accuracy: 0.8599
# Epoch: 7, Accuracy: 0.8673
# Epoch: 8, Accuracy: 0.8706
# Epoch: 9, Accuracy: 0.8746

# TODO L2 vs dropout na 500 epochÃ³w